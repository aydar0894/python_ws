{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats.stats import pearsonr # used to calculate correlation coefficient\n",
    "from pymongo import MongoClient\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "from pprint import pprint\n",
    "from enum import Enum\n",
    "import copy\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read timeseries of benchmark and coin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        timestamp      open      high       low     close  \\\n",
      "date                                                                        \n",
      "2018-01-01 00:00:00  1.514765e+09   9830.51   9830.51   9830.51   9830.51   \n",
      "2018-01-01 01:00:00  1.514768e+09   9830.42   9830.42   9830.42   9830.42   \n",
      "2018-01-01 02:00:00  1.514772e+09   9874.93   9874.93   9874.93   9874.93   \n",
      "2018-01-01 03:00:00  1.514776e+09   9894.21   9894.21   9894.21   9894.21   \n",
      "2018-01-01 04:00:00  1.514779e+09   9900.78   9900.78   9900.78   9900.78   \n",
      "2018-01-01 05:00:00  1.514783e+09  10032.37  10032.37  10032.37  10032.37   \n",
      "2018-01-01 06:00:00  1.514786e+09  10120.35  10120.35  10120.35  10120.35   \n",
      "2018-01-01 07:00:00  1.514790e+09  10120.25  10120.25  10120.25  10120.25   \n",
      "2018-01-01 08:00:00  1.514794e+09  10070.72  10070.72  10070.72  10070.72   \n",
      "2018-01-01 09:00:00  1.514797e+09  10091.04  10091.04  10091.04  10091.04   \n",
      "2018-01-01 10:00:00  1.514801e+09  10170.63  10170.63  10170.63  10170.63   \n",
      "2018-01-01 11:00:00  1.514804e+09  10253.40  10253.40  10253.40  10253.40   \n",
      "2018-01-01 12:00:00  1.514808e+09  10193.89  10193.89  10193.89  10193.89   \n",
      "2018-01-01 13:00:00  1.514812e+09  10094.48  10094.48  10094.48  10094.48   \n",
      "2018-01-01 14:00:00  1.514815e+09  10058.01  10058.01  10058.01  10058.01   \n",
      "2018-01-01 15:00:00  1.514819e+09  10101.63  10101.63  10101.63  10101.63   \n",
      "2018-01-01 16:00:00  1.514822e+09  10041.46  10041.46  10041.46  10041.46   \n",
      "2018-01-01 17:00:00  1.514826e+09   9929.96   9929.96   9929.96   9929.96   \n",
      "2018-01-01 18:00:00  1.514830e+09  10046.39  10046.39  10046.39  10046.39   \n",
      "2018-01-01 19:00:00  1.514833e+09   9946.28   9946.28   9946.28   9946.28   \n",
      "2018-01-01 20:00:00  1.514837e+09   9982.55   9982.55   9982.55   9982.55   \n",
      "2018-01-01 21:00:00  1.514840e+09   9967.75   9967.75   9967.75   9967.75   \n",
      "2018-01-01 22:00:00  1.514844e+09  10026.06  10026.06  10026.06  10026.06   \n",
      "2018-01-01 23:00:00  1.514848e+09   9963.67   9963.67   9963.67   9963.67   \n",
      "2018-01-02 00:00:00  1.514851e+09  10026.23  10026.23  10026.23  10026.23   \n",
      "2018-01-02 01:00:00  1.514855e+09  10062.24  10062.24  10062.24  10062.24   \n",
      "2018-01-02 02:00:00  1.514858e+09  10073.56  10073.56  10073.56  10073.56   \n",
      "2018-01-02 03:00:00  1.514862e+09  10001.90  10001.90  10001.90  10001.90   \n",
      "2018-01-02 04:00:00  1.514866e+09  10012.65  10012.65  10012.65  10012.65   \n",
      "2018-01-02 05:00:00  1.514869e+09  10056.90  10056.90  10056.90  10056.90   \n",
      "...                           ...       ...       ...       ...       ...   \n",
      "2018-01-30 21:00:00  1.517346e+09  18518.03  18518.03  18518.03  18518.03   \n",
      "2018-01-30 22:00:00  1.517350e+09  18440.39  18440.39  18440.39  18440.39   \n",
      "2018-01-30 23:00:00  1.517353e+09  18456.42  18456.42  18456.42  18456.42   \n",
      "2018-01-31 00:00:00  1.517357e+09  18472.59  18472.59  18472.59  18472.59   \n",
      "2018-01-31 01:00:00  1.517360e+09  18592.40  18592.40  18592.40  18592.40   \n",
      "2018-01-31 02:00:00  1.517364e+09  18822.98  18822.98  18822.98  18822.98   \n",
      "2018-01-31 03:00:00  1.517368e+09  18722.87  18722.87  18722.87  18722.87   \n",
      "2018-01-31 04:00:00  1.517371e+09  18673.42  18673.42  18673.42  18673.42   \n",
      "2018-01-31 05:00:00  1.517375e+09  18764.37  18764.37  18764.37  18764.37   \n",
      "2018-01-31 06:00:00  1.517378e+09  18606.55  18606.55  18606.55  18606.55   \n",
      "2018-01-31 07:00:00  1.517382e+09  18460.04  18460.04  18460.04  18460.04   \n",
      "2018-01-31 08:00:00  1.517386e+09  18456.69  18456.69  18456.69  18456.69   \n",
      "2018-01-31 09:00:00  1.517389e+09  18331.14  18331.14  18331.14  18331.14   \n",
      "2018-01-31 10:00:00  1.517393e+09  18562.45  18562.45  18562.45  18562.45   \n",
      "2018-01-31 11:00:00  1.517396e+09  18505.01  18505.01  18505.01  18505.01   \n",
      "2018-01-31 12:00:00  1.517400e+09  18407.60  18407.60  18407.60  18407.60   \n",
      "2018-01-31 13:00:00  1.517404e+09  18539.04  18539.04  18539.04  18539.04   \n",
      "2018-01-31 14:00:00  1.517407e+09  18702.19  18702.19  18702.19  18702.19   \n",
      "2018-01-31 15:00:00  1.517411e+09  18689.93  18689.93  18689.93  18689.93   \n",
      "2018-01-31 16:00:00  1.517414e+09  18775.24  18775.24  18775.24  18775.24   \n",
      "2018-01-31 17:00:00  1.517418e+09  18665.65  18665.65  18665.65  18665.65   \n",
      "2018-01-31 18:00:00  1.517422e+09  18614.24  18614.24  18614.24  18614.24   \n",
      "2018-01-31 19:00:00  1.517425e+09  18786.17  18786.17  18786.17  18786.17   \n",
      "2018-01-31 20:00:00  1.517429e+09  18760.78  18760.78  18760.78  18760.78   \n",
      "2018-01-31 21:00:00  1.517432e+09  18820.15  18820.15  18820.15  18820.15   \n",
      "2018-01-31 22:00:00  1.517436e+09  18785.73  18785.73  18785.73  18785.73   \n",
      "2018-01-31 23:00:00  1.517440e+09  18675.48  18675.48  18675.48  18675.48   \n",
      "NaT                           NaN       NaN       NaN       NaN       NaN   \n",
      "NaT                           NaN       NaN       NaN       NaN       NaN   \n",
      "NaT                           NaN       NaN       NaN       NaN       NaN   \n",
      "\n",
      "                     volFrom  volTo  \n",
      "date                                 \n",
      "2018-01-01 00:00:00      0.0    0.0  \n",
      "2018-01-01 01:00:00      0.0    0.0  \n",
      "2018-01-01 02:00:00      0.0    0.0  \n",
      "2018-01-01 03:00:00      0.0    0.0  \n",
      "2018-01-01 04:00:00      0.0    0.0  \n",
      "2018-01-01 05:00:00      0.0    0.0  \n",
      "2018-01-01 06:00:00      0.0    0.0  \n",
      "2018-01-01 07:00:00      0.0    0.0  \n",
      "2018-01-01 08:00:00      0.0    0.0  \n",
      "2018-01-01 09:00:00      0.0    0.0  \n",
      "2018-01-01 10:00:00      0.0    0.0  \n",
      "2018-01-01 11:00:00      0.0    0.0  \n",
      "2018-01-01 12:00:00      0.0    0.0  \n",
      "2018-01-01 13:00:00      0.0    0.0  \n",
      "2018-01-01 14:00:00      0.0    0.0  \n",
      "2018-01-01 15:00:00      0.0    0.0  \n",
      "2018-01-01 16:00:00      0.0    0.0  \n",
      "2018-01-01 17:00:00      0.0    0.0  \n",
      "2018-01-01 18:00:00      0.0    0.0  \n",
      "2018-01-01 19:00:00      0.0    0.0  \n",
      "2018-01-01 20:00:00      0.0    0.0  \n",
      "2018-01-01 21:00:00      0.0    0.0  \n",
      "2018-01-01 22:00:00      0.0    0.0  \n",
      "2018-01-01 23:00:00      0.0    0.0  \n",
      "2018-01-02 00:00:00      0.0    0.0  \n",
      "2018-01-02 01:00:00      0.0    0.0  \n",
      "2018-01-02 02:00:00      0.0    0.0  \n",
      "2018-01-02 03:00:00      0.0    0.0  \n",
      "2018-01-02 04:00:00      0.0    0.0  \n",
      "2018-01-02 05:00:00      0.0    0.0  \n",
      "...                      ...    ...  \n",
      "2018-01-30 21:00:00      0.0    0.0  \n",
      "2018-01-30 22:00:00      0.0    0.0  \n",
      "2018-01-30 23:00:00      0.0    0.0  \n",
      "2018-01-31 00:00:00      0.0    0.0  \n",
      "2018-01-31 01:00:00      0.0    0.0  \n",
      "2018-01-31 02:00:00      0.0    0.0  \n",
      "2018-01-31 03:00:00      0.0    0.0  \n",
      "2018-01-31 04:00:00      0.0    0.0  \n",
      "2018-01-31 05:00:00      0.0    0.0  \n",
      "2018-01-31 06:00:00      0.0    0.0  \n",
      "2018-01-31 07:00:00      0.0    0.0  \n",
      "2018-01-31 08:00:00      0.0    0.0  \n",
      "2018-01-31 09:00:00      0.0    0.0  \n",
      "2018-01-31 10:00:00      0.0    0.0  \n",
      "2018-01-31 11:00:00      0.0    0.0  \n",
      "2018-01-31 12:00:00      0.0    0.0  \n",
      "2018-01-31 13:00:00      0.0    0.0  \n",
      "2018-01-31 14:00:00      0.0    0.0  \n",
      "2018-01-31 15:00:00      0.0    0.0  \n",
      "2018-01-31 16:00:00      0.0    0.0  \n",
      "2018-01-31 17:00:00      0.0    0.0  \n",
      "2018-01-31 18:00:00      0.0    0.0  \n",
      "2018-01-31 19:00:00      0.0    0.0  \n",
      "2018-01-31 20:00:00      0.0    0.0  \n",
      "2018-01-31 21:00:00      0.0    0.0  \n",
      "2018-01-31 22:00:00      0.0    0.0  \n",
      "2018-01-31 23:00:00      0.0    0.0  \n",
      "NaT                      NaN    NaN  \n",
      "NaT                      NaN    NaN  \n",
      "NaT                      NaN    NaN  \n",
      "\n",
      "[747 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "filename_benchmark = 'testing/BTC_Bitfinex_USD.csv'\n",
    "filename_coin      = 'testing/XRP_Bitfinex_USD.csv'\n",
    "\n",
    "# --- read benchmark ---\n",
    "df_benchmark = pd.read_csv(filename_benchmark, header=None, sep=\";\", \n",
    "                            names=['date', 'timestamp', 'open', 'high', 'low', 'close', 'volFrom', 'volTo'])\n",
    "    \n",
    "# this makes indexing via date faster\n",
    "df_benchmark = df_benchmark.set_index(['date'])         # index: string\n",
    "df_benchmark.index = pd.to_datetime(df_benchmark.index) # index: datetime\n",
    "\n",
    "pprint(df_benchmark)\n",
    "\n",
    "# --- read coin ---\n",
    "df_coin = pd.read_csv(filename_coin, header=None, sep=\";\", \n",
    "                      names=['date', 'timestamp', 'open', 'high', 'low', 'close', 'volFrom', 'volTo'])\n",
    "    \n",
    "# this makes indexing via date faster\n",
    "df_coin = df_coin.set_index(['date'])         # index: string\n",
    "df_coin.index = pd.to_datetime(df_coin.index) # index: datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Multiplier and Correlation\n",
    "\n",
    "If there is no pre-defined libraries to compute (Pearson) correlation, it can also be calculated by its definition: <br>\n",
    "https://en.wikipedia.org/wiki/Pearson_correlation_coefficient\n",
    "\n",
    "This relies on calculating the covariance between two vectors, and the variance of each vector indidually. <br>\n",
    "If you need further clarification please ask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multiplier            :  -0.6945408340863843\n",
      "correlation            : -0.15598094157469428\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "\n",
    "# input variables\n",
    "\n",
    "#input parameter\n",
    "dt_benchmark_startTime    = datetime.datetime.strptime(\"2018-01-01 00:00:00\", \"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "#always current time\n",
    "dt_benchmark_endTime      = datetime.datetime.strptime(\"2018-01-31 23:00:00\", \"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "#input parameter\n",
    "ReturnFrequency = \"daily\"\n",
    "\n",
    "#list of currencies\n",
    "\n",
    "#END input variables\n",
    "    \n",
    "dt_currentTime = dt_benchmark_startTime\n",
    "\n",
    "# add first interval\n",
    "dt_previousTime = dt_currentTime\n",
    "if ReturnFrequency == \"hourly\":\n",
    "    dt_currentTime += datetime.timedelta(hours=1)\n",
    "elif ReturnFrequency == \"daily\":\n",
    "    dt_currentTime += datetime.timedelta(days=1)\n",
    "else:\n",
    "    print('ERROR. Need to implment other frequencies')\n",
    "    assert(False)\n",
    "\n",
    "\n",
    "arr_PnL_benchmark = np.array([])\n",
    "arr_PnL_coin       = np.array([])\n",
    "   \n",
    "#-----------------------------------------#\n",
    "#          calculate return timeseries    #   \n",
    "#-----------------------------------------#\n",
    "\n",
    "while (dt_currentTime <= dt_benchmark_endTime):\n",
    "    # calculate return of benchmark in period [t-1, t]\n",
    "    PnL_benchmark = df_benchmark.loc[dt_currentTime]['close'] / \\\n",
    "                    df_benchmark.loc[dt_previousTime]['close'] -1.0\n",
    "    arr_PnL_benchmark = np.append(arr_PnL_benchmark, PnL_benchmark)\n",
    "    \n",
    "    # calculate return of strategy in period [t-1, t] (based on equity, i.e. MtM value of positions)\n",
    "    PnL_coin = df_coin.loc[dt_currentTime]['close']  / \\\n",
    "               df_coin.loc[dt_previousTime]['close'] -1.0\n",
    "    arr_PnL_coin = np.append(arr_PnL_coin, PnL_coin)\n",
    "     \n",
    "    # move to next timepoint\n",
    "    if ReturnFrequency == \"hourly\":\n",
    "        dt_previousTime += datetime.timedelta(hours=1)\n",
    "        dt_currentTime  += datetime.timedelta(hours=1)\n",
    "    elif ReturnFrequency == \"daily\":\n",
    "        dt_previousTime += datetime.timedelta(days=1)\n",
    "        dt_currentTime += datetime.timedelta(days=1)\n",
    "    else:\n",
    "        print('ERROR. Need to implment other frequencies')\n",
    "        assert(False)\n",
    "\n",
    "#-----------------------------------------#\n",
    "#          calculate multiplier           #   \n",
    "#-----------------------------------------#\n",
    "arr_x = arr_PnL_benchmark\n",
    "arr_y = arr_PnL_coin\n",
    "\n",
    "# least square regression (linear): y = alpha + beta*x\n",
    "linReg = np.polyfit(x=arr_PnL_benchmark, y=arr_PnL_coin, deg=1)\n",
    "\n",
    "alpha = linReg[1] # this is the y-intercept, not needed\n",
    "beta  = linReg[0] # this is the slope, which also is the multiplier\n",
    "multiplier = beta\n",
    "print(\"multiplier            : \", multiplier)\n",
    "\n",
    "#-----------------------------------------#\n",
    "#          calculate correlation          #   \n",
    "#-----------------------------------------#\n",
    "correlation = pearsonr(arr_PnL_benchmark, arr_PnL_coin)\n",
    "print(\"correlation            :\", correlation[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Multiplier and Correlation class calculator\n",
    "\n",
    "Class calculates multiplier and correlation matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiplierCorellationCalculator:\n",
    "    class RequestFrequency(Enum):\n",
    "        DAILY  = 0\n",
    "        HOURLY = 1\n",
    "    FREQUENCY_LIST = RequestFrequency.__members__.items()\n",
    "    \n",
    "    def __init__(self,\n",
    "                 start_time, \n",
    "                 end_time,\n",
    "                 currencies_list, \n",
    "                 return_frequency='daily'):\n",
    "        self.start_time = start_time\n",
    "        self.end_time         = end_time\n",
    "        self.currencies_list  = currencies_list\n",
    "        self.return_frequency = return_frequency\n",
    "\n",
    "    \n",
    "    def calculate_aggregated_pairs(self):\n",
    "        currencies_list  = deque(self.currencies_list)\n",
    "        pairs_multiplier_correlation = {}\n",
    "        while len(currencies_list) > 1:\n",
    "            benchmark_currency  = currencies_list.popleft()\n",
    "            for coin_currency in currencies_list:\n",
    "                pair_tag =  \"%s/%s\" % (benchmark_currency, coin_currency)\n",
    "                multiplier, correlation = self.calculation_for_pair(benchmark_currency, coin_currency)\n",
    "                pairs_multiplier_correlation[pair_tag] = { 'multiplier': multiplier, \n",
    "                                                          'correlation': correlation }\n",
    "        return pairs_multiplier_correlation\n",
    "        \n",
    "        \n",
    "    def calculation_for_pair(self, benchmark_tag, coin_tag):\n",
    "        # --- read coin ---\n",
    "        arr_PnL_benchmark, arr_PnL_coin = self._calculate_timeseries()\n",
    "        multiplier, correlation         = self._calculate_multiplier_and_correlation(arr_PnL_benchmark, \n",
    "                                                                                     arr_PnL_coin)\n",
    "        return (multiplier, correlation)\n",
    "    \n",
    "    def _calculate_multiplier_and_correlation(self, arr_PnL_benchmark, arr_PnL_coin):\n",
    "        # arr_x = arr_PnL_benchmark\n",
    "        # arr_y = arr_PnL_coin\n",
    "        #          calculate multiplier           #  \n",
    "        # least square regression (linear): y = alpha + beta*x\n",
    "        linReg = np.polyfit(x=arr_PnL_benchmark, y=arr_PnL_coin, deg=1)\n",
    "        alpha = linReg[1] # this is the y-intercept, not needed\n",
    "        beta  = linReg[0] # this is the slope, which also is the multiplier\n",
    "        multiplier = beta\n",
    "        print(\"multiplier            : \", multiplier)\n",
    "        #          calculate correlation          #   \n",
    "        correlation = pearsonr(arr_PnL_benchmark, arr_PnL_coin)\n",
    "        print(\"correlation            :\", correlation[0])\n",
    "        return (multiplier, correlation[0])\n",
    "        \n",
    "    #-----------------------------------------#\n",
    "    #          calculate return timeseries    # \n",
    "    #-----------------------------------------#\n",
    "    def _calculate_timeseries(self):\n",
    "        dt_previousTime    = copy.deepcopy(self.start_time)\n",
    "        dt_currentTime     = copy.deepcopy(self.start_time)\n",
    "        # add first interval\n",
    "        dt_currentTime,     = self._increment_interval(dt_currentTime)\n",
    "\n",
    "        df_benchmark = self._retrieve_currency_history(benchmark_ccy)\n",
    "        df_coin = self._retrieve_currency_history(coin_ccy)\n",
    "\n",
    "        arr_PnL_benchmark = np.array([])\n",
    "        arr_PnL_coin       = np.array([])\n",
    "        \n",
    "        while (dt_currentTime <= self.end_time):\n",
    "            # calculate return of benchmark in period [t-1, t]\n",
    "            arr_PnL_benchmark = self._calculate_PnL(arr_PnL_benchmark,\n",
    "                                                df_benchmark, \n",
    "                                                dt_currentTime, \n",
    "                                                dt_previousTime)\n",
    "            arr_PnL_coin      = self._calculate_PnL(arr_PnL_coin,\n",
    "                                                df_coin, \n",
    "                                                dt_currentTime, \n",
    "                                                dt_previousTime)\n",
    "            # move to next timepoint\n",
    "            dt_previousTime, dt_currentTime = self._increment_interval(dt_previousTime, \n",
    "                                                                       dt_currentTime)\n",
    "        return (arr_PnL_benchmark, arr_PnL_coin)\n",
    "\n",
    "\n",
    "    def _calculate_PnL(self, arr_PnL, df_data, dt_currentTime, dt_previousTime):\n",
    "        # calculate return of strategy in period [t-1, t] (based on equity, i.e. MtM value of positions)\n",
    "        PnL = df_data.loc[dt_currentTime]['close']  / \\\n",
    "                    df_data.loc[dt_previousTime]['close'] -1.0\n",
    "        arr_PnL = np.append(arr_PnL, PnL)\n",
    "        return arr_PnL\n",
    "    \n",
    "    def _increment_interval(self, *date_time_fields):\n",
    "        if self.return_frequency == 'daily':\n",
    "            return map(lambda dt: dt + timedelta(days=1), date_time_fields)\n",
    "        elif self.return_frequency == 'hourly':\n",
    "            return map(lambda dt: dt + timedelta(hours=1), date_time_fields)\n",
    "        else:\n",
    "            print('ERROR. Need to implment other frequencies')\n",
    "            assert(False)\n",
    "\n",
    "    # --- connect and preprocess utilities for mongo collection --- \n",
    "    def _reconstruct_currency_date(self, cur):\n",
    "        for cur_value, index in zip(cur['history'], range(len(cur['history']))):\n",
    "#             cur['history'][index]['date'] = datetime.fromtimestamp(cur_value['time'])\n",
    "            cur['history'][index]['date'] = \"{:%Y-%m-%d %H:%M:%S}\".format(datetime.fromtimestamp(cur_value['time']))\n",
    "        return cur\n",
    "\n",
    "    \n",
    "    def _mongo_connect(self, db_name):\n",
    "        mongo_c = MongoClient()\n",
    "        db = mongo_c[db_name]\n",
    "        if db:\n",
    "            return db\n",
    "        else:\n",
    "            raise Exception(\"database or server not found\")\n",
    "        \n",
    "        \n",
    "    def _preprocess_collection(self, collection_name, filter_params):\n",
    "        db = self._mongo_connect('darqube_db')       \n",
    "        collection = db[collection_name]    \n",
    "        if not collection:\n",
    "            raise Exception('collection not found')\n",
    "        return self._reconstruct_currency_date(collection.find_one(filter_params))\n",
    "\n",
    "    \n",
    "    def _retrieve_currency_history(self, currency):\n",
    "        collection = self._preprocess_collection('currencies_collection', {'Ccy': currency})\n",
    "        df_data = pd.DataFrame(collection['history'])\n",
    "        # this makes indexing via date faster\n",
    "        df_data = df_data.set_index(['date'])         # index: string\n",
    "        df_data.index = pd.to_datetime(df_data.index)\n",
    "        return df_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multiplier            :  0.6886096420205688\n",
      "correlation            : 0.35501628723680734\n",
      "{'BTC/ETH': {'multiplier': 0.6886096420205688, 'correlation': 0.35501628723680734}}\n"
     ]
    }
   ],
   "source": [
    "dt_benchmark_startTime    = datetime.strptime(\"2015-08-07 03:00:00\", \"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "#always current time\n",
    "dt_benchmark_endTime      = datetime.strptime(\"2018-10-08 03:00:00\", \"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "currencies_list = ['BTC', 'ETH']\n",
    "return_frequency = 'daily'\n",
    "\n",
    "new_compare = MultiplierCorellationCalculator(dt_benchmark_startTime, dt_benchmark_endTime, currencies_list)\n",
    "print(new_compare.calculate_aggregated_pairs())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
